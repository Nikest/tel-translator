require('dotenv').config();
const WebSocket = require('ws');
const http = require('http');
const url = require('url');
const fs = require('fs');
const path = require('path');

const PORT = process.env.PORT || 8080;
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const OPENAI_MODEL = 'gpt-4o-realtime-preview-2024-12-17';

let waitingOperator = null;

// --- HTTP Server ---
const server = http.createServer((req, res) => {
    if (req.method === 'GET' && (req.url === '/' || req.url === '/index.html')) {
        const filePath = path.join(__dirname, 'index.html');
        fs.readFile(filePath, (err, content) => {
            if (err) {
                res.writeHead(500);
                res.end('Error loading index.html');
            } else {
                res.writeHead(200, { 'Content-Type': 'text/html' });
                res.end(content);
            }
        });
    } else {
        res.writeHead(200);
        res.end('Translator Bridge Running');
    }
});

// --- WebSocket Server ---
const wss = new WebSocket.Server({ noServer: true });

server.on('upgrade', (request, socket, head) => {
    const pathname = url.parse(request.url).pathname;

    if (pathname === '/call') {
        wss.handleUpgrade(request, socket, head, (ws) => {
            ws.clientType = 'phone';
            wss.emit('connection', ws, request);
        });
    } else if (pathname === '/operator') {
        wss.handleUpgrade(request, socket, head, (ws) => {
            ws.clientType = 'operator';
            wss.emit('connection', ws, request);
        });
    } else {
        socket.destroy();
    }
});

wss.on('connection', (ws) => {
    // --- ÐžÐ¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð¸Ð»ÑÑ ---
    if (ws.clientType === 'operator') {
        console.log('[Operator] âœ“ Connected via browser');
        waitingOperator = ws;

        ws.send(JSON.stringify({ type: 'status', msg: 'Waiting for a call...' }));

        ws.on('close', () => {
            console.log('[Operator] Disconnected');
            if (waitingOperator === ws) waitingOperator = null;
        });

        ws.on('error', (err) => {
            console.error('[Operator] âŒ Error:', err.message);
        });

        return;
    }

    // --- Ð—Ð²Ð¾Ð½Ð¾Ðº Ð¾Ñ‚ Ð°Ð±Ð¾Ð½ÐµÐ½Ñ‚Ð° ---
    console.log('[Phone] ðŸ“ž Incoming call from SignalWire');

    if (!waitingOperator || waitingOperator.readyState !== WebSocket.OPEN) {
        console.log('[System] âŒ No operator available, rejecting call');
        ws.close();
        return;
    }

    const operatorWs = waitingOperator;
    waitingOperator = null;

    operatorWs.send(JSON.stringify({ type: 'status', msg: 'Call Connected!' }));
    console.log('[System] âœ“ Bridge created: Phone â†” Operator');

    startTranslationSession(ws, operatorWs);
});

// --- Translation Session ---
function startTranslationSession(phoneWs, operatorWs) {
    let streamSid = null;

    // Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð´Ð»Ñ RUâ†’EN
    let ruToEnReady = false;
    let ruToEnQueue = [];

    // Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð´Ð»Ñ ENâ†’RU
    let enToRuReady = false;
    let enToRuQueue = [];

    // --- OpenAI: Russian â†’ English (Ð´Ð»Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð°) ---
    const ai_RuToEn = new WebSocket(`wss://api.openai.com/v1/realtime?model=${OPENAI_MODEL}`, {
        headers: {
            Authorization: `Bearer ${OPENAI_API_KEY}`,
            "OpenAI-Beta": "realtime=v1"
        }
    });

    // --- OpenAI: English â†’ Russian (Ð´Ð»Ñ Ð°Ð±Ð¾Ð½ÐµÐ½Ñ‚Ð°) ---
    const ai_EnToRu = new WebSocket(`wss://api.openai.com/v1/realtime?model=${OPENAI_MODEL}`, {
        headers: {
            Authorization: `Bearer ${OPENAI_API_KEY}`,
            "OpenAI-Beta": "realtime=v1"
        }
    });

    // =====================
    // RU â†’ EN (Phone â†’ Operator)
    // =====================
    ai_RuToEn.on('open', () => {
        console.log('[OpenAI RUâ†’EN] âœ“ Connected');

        const config = {
            type: 'session.update',
            session: {
                modalities: ['audio', 'text'],
                instructions: 'You are a real-time interpreter. Translate Russian speech to English. Output only the English translation, nothing else. No commentary, no explanations.',
                voice: 'alloy',
                input_audio_format: 'g711_ulaw',
                output_audio_format: 'pcm16',
                turn_detection: {
                    type: 'server_vad',
                    threshold: 0.5,
                    prefix_padding_ms: 300,
                    silence_duration_ms: 200
                },
                input_audio_transcription: {
                    model: 'whisper-1'
                },
                temperature: 0.6
            }
        };
        ai_RuToEn.send(JSON.stringify(config));
    });

    ai_RuToEn.on('message', (data) => {
        try {
            const response = JSON.parse(data);

            if (response.type === 'session.updated') {
                ruToEnReady = true;
                console.log('[OpenAI RUâ†’EN] âœ“ Session ready');

                // ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð½Ð°ÐºÐ¾Ð¿Ð»ÐµÐ½Ð½ÑƒÑŽ Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ
                if (ruToEnQueue.length > 0) {
                    console.log(`[OpenAI RUâ†’EN] â†’ Flushing ${ruToEnQueue.length} queued chunks`);
                    ruToEnQueue.forEach(audio => ai_RuToEn.send(JSON.stringify(audio)));
                    ruToEnQueue = [];
                }
            }

            if (response.type === 'input_audio_buffer.speech_started') {
                console.log('[Phone] ðŸŽ¤ Speaking...');
            }

            if (response.type === 'conversation.item.input_audio_transcription.completed') {
                console.log(`[Phone] ðŸ“ "${response.transcript}"`);
            }

            if (response.type === 'response.audio.delta' && response.delta) {
                // ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ PCM16 Ð°ÑƒÐ´Ð¸Ð¾ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ñƒ
                if (operatorWs.readyState === WebSocket.OPEN) {
                    operatorWs.send(JSON.stringify({
                        type: 'audio',
                        payload: response.delta
                    }));
                }
            }

            if (response.type === 'response.audio_transcript.done') {
                console.log(`[AIâ†’Operator] ðŸ”Š "${response.transcript}"`);
            }

            if (response.type === 'error') {
                if (response.error?.code !== 'input_audio_buffer_commit_empty') {
                    console.error('[OpenAI RUâ†’EN] âŒ', response.error?.message);
                }
            }

        } catch (e) {
            console.error('[OpenAI RUâ†’EN] âŒ Parse error:', e.message);
        }
    });

    ai_RuToEn.on('error', (e) => console.error('[OpenAI RUâ†’EN] âŒ WS Error:', e.message));
    ai_RuToEn.on('close', () => console.log('[OpenAI RUâ†’EN] Connection closed'));

    // =====================
    // EN â†’ RU (Operator â†’ Phone)
    // =====================
    ai_EnToRu.on('open', () => {
        console.log('[OpenAI ENâ†’RU] âœ“ Connected');

        const config = {
            type: 'session.update',
            session: {
                modalities: ['audio', 'text'],
                instructions: 'You are a real-time interpreter. Translate English speech to Russian. Output only the Russian translation, nothing else. No commentary, no explanations.',
                voice: 'echo',
                input_audio_format: 'pcm16',
                output_audio_format: 'g711_ulaw',
                turn_detection: {
                    type: 'server_vad',
                    threshold: 0.5,
                    prefix_padding_ms: 300,
                    silence_duration_ms: 200
                },
                input_audio_transcription: {
                    model: 'whisper-1'
                },
                temperature: 0.6
            }
        };
        ai_EnToRu.send(JSON.stringify(config));
    });

    ai_EnToRu.on('message', (data) => {
        try {
            const response = JSON.parse(data);

            if (response.type === 'session.updated') {
                enToRuReady = true;
                console.log('[OpenAI ENâ†’RU] âœ“ Session ready');

                // ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð½Ð°ÐºÐ¾Ð¿Ð»ÐµÐ½Ð½ÑƒÑŽ Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ
                if (enToRuQueue.length > 0) {
                    console.log(`[OpenAI ENâ†’RU] â†’ Flushing ${enToRuQueue.length} queued chunks`);
                    enToRuQueue.forEach(audio => ai_EnToRu.send(JSON.stringify(audio)));
                    enToRuQueue = [];
                }
            }

            if (response.type === 'input_audio_buffer.speech_started') {
                console.log('[Operator] ðŸŽ¤ Speaking...');
            }

            if (response.type === 'conversation.item.input_audio_transcription.completed') {
                console.log(`[Operator] ðŸ“ "${response.transcript}"`);
            }

            if (response.type === 'response.audio.delta' && response.delta) {
                // ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ G.711 Ð°ÑƒÐ´Ð¸Ð¾ Ð½Ð° Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½
                if (streamSid && phoneWs.readyState === WebSocket.OPEN) {
                    phoneWs.send(JSON.stringify({
                        event: 'media',
                        streamSid: streamSid,
                        media: { payload: response.delta }
                    }));
                }
            }

            if (response.type === 'response.audio_transcript.done') {
                console.log(`[AIâ†’Phone] ðŸ”Š "${response.transcript}"`);
            }

            if (response.type === 'error') {
                if (response.error?.code !== 'input_audio_buffer_commit_empty') {
                    console.error('[OpenAI ENâ†’RU] âŒ', response.error?.message);
                }
            }

        } catch (e) {
            console.error('[OpenAI ENâ†’RU] âŒ Parse error:', e.message);
        }
    });

    ai_EnToRu.on('error', (e) => console.error('[OpenAI ENâ†’RU] âŒ WS Error:', e.message));
    ai_EnToRu.on('close', () => console.log('[OpenAI ENâ†’RU] Connection closed'));

    // =====================
    // ÐœÐ°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð°ÑƒÐ´Ð¸Ð¾
    // =====================

    // ÐžÑ‚ Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½Ð° â†’ OpenAI RUâ†’EN
    phoneWs.on('message', (message) => {
        try {
            const msg = JSON.parse(message);

            if (msg.event === 'start') {
                streamSid = msg.start.streamSid;
                console.log(`[Phone] âœ“ Stream started: ${streamSid.substring(0, 8)}...`);
            }

            if (msg.event === 'media') {
                const audioData = {
                    type: 'input_audio_buffer.append',
                    audio: msg.media.payload
                };

                if (ai_RuToEn.readyState === WebSocket.OPEN) {
                    if (ruToEnReady) {
                        ai_RuToEn.send(JSON.stringify(audioData));
                    } else {
                        ruToEnQueue.push(audioData);
                    }
                }
            }

            if (msg.event === 'stop') {
                console.log('[Phone] Call ended');
                closeAll();
            }

        } catch (e) {
            // Ð˜Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
        }
    });

    // ÐžÑ‚ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð° â†’ OpenAI ENâ†’RU
    operatorWs.on('message', (message) => {
        try {
            const msg = JSON.parse(message);

            if (msg.type === 'audio') {
                const audioData = {
                    type: 'input_audio_buffer.append',
                    audio: msg.payload
                };

                if (ai_EnToRu.readyState === WebSocket.OPEN) {
                    if (enToRuReady) {
                        ai_EnToRu.send(JSON.stringify(audioData));
                    } else {
                        enToRuQueue.push(audioData);
                    }
                }
            }

        } catch (e) {
            // Ð˜Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
        }
    });

    // =====================
    // Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ ÑÐµÑÑÐ¸Ð¸
    // =====================
    const closeAll = () => {
        console.log('[System] Closing all connections...');

        if (ai_RuToEn.readyState === WebSocket.OPEN) ai_RuToEn.close();
        if (ai_EnToRu.readyState === WebSocket.OPEN) ai_EnToRu.close();
        if (phoneWs.readyState === WebSocket.OPEN) phoneWs.close();

        // Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð° Ð² Ñ€ÐµÐ¶Ð¸Ð¼ Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ñ (Ð½Ðµ Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ ÐµÐ³Ð¾ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ!)
        if (operatorWs.readyState === WebSocket.OPEN) {
            operatorWs.send(JSON.stringify({ type: 'status', msg: 'Waiting for a call...' }));
            waitingOperator = operatorWs;
            console.log('[System] âœ“ Operator returned to waiting state');
        }
    };

    phoneWs.on('close', closeAll);
    phoneWs.on('error', (err) => {
        console.error('[Phone] âŒ Error:', err.message);
        closeAll();
    });

    operatorWs.on('close', () => {
        console.log('[Operator] Disconnected during call');
        if (ai_RuToEn.readyState === WebSocket.OPEN) ai_RuToEn.close();
        if (ai_EnToRu.readyState === WebSocket.OPEN) ai_EnToRu.close();
        if (phoneWs.readyState === WebSocket.OPEN) phoneWs.close();
    });
}

// --- Start Server ---
server.listen(PORT, () => {
    console.log(`ðŸš€ Translator Bridge running on port ${PORT}`);
    console.log(`ðŸ“ž Phone endpoint: /call`);
    console.log(`ðŸ’» Operator endpoint: /operator`);
    console.log(`ðŸ”— Model: ${OPENAI_MODEL}`);
});